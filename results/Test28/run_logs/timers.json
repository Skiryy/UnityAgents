{
    "name": "root",
    "gauges": {
        "MoveToTarget.Policy.Entropy.mean": {
            "value": 1.4088941812515259,
            "min": 1.4088941812515259,
            "max": 1.4291397333145142,
            "count": 41
        },
        "MoveToTarget.Policy.Entropy.sum": {
            "value": 70479.9296875,
            "min": 70374.265625,
            "max": 71521.296875,
            "count": 41
        },
        "MoveToTarget.Step.mean": {
            "value": 2049964.0,
            "min": 49937.0,
            "max": 2049964.0,
            "count": 41
        },
        "MoveToTarget.Step.sum": {
            "value": 2049964.0,
            "min": 49937.0,
            "max": 2049964.0,
            "count": 41
        },
        "MoveToTarget.Policy.ExtrinsicValueEstimate.mean": {
            "value": 0.02068362385034561,
            "min": 0.017497316002845764,
            "max": 1.435874342918396,
            "count": 41
        },
        "MoveToTarget.Policy.ExtrinsicValueEstimate.sum": {
            "value": 16.15390968322754,
            "min": 13.665403366088867,
            "max": 1127.161376953125,
            "count": 41
        },
        "MoveToTarget.Environment.EpisodeLength.mean": {
            "value": 1608.142857142857,
            "min": 337.90277777777777,
            "max": 1608.142857142857,
            "count": 3
        },
        "MoveToTarget.Environment.EpisodeLength.sum": {
            "value": 11257.0,
            "min": 11257.0,
            "max": 50900.0,
            "count": 3
        },
        "MoveToTarget.Environment.CumulativeReward.mean": {
            "value": -2.0,
            "min": -2.0,
            "max": -1.6666666666666667,
            "count": 3
        },
        "MoveToTarget.Environment.CumulativeReward.sum": {
            "value": -14.0,
            "min": -130.0,
            "max": -14.0,
            "count": 3
        },
        "MoveToTarget.Policy.ExtrinsicReward.mean": {
            "value": -2.0,
            "min": -2.0,
            "max": -1.6666666666666667,
            "count": 3
        },
        "MoveToTarget.Policy.ExtrinsicReward.sum": {
            "value": -14.0,
            "min": -130.0,
            "max": -14.0,
            "count": 3
        },
        "MoveToTarget.Losses.PolicyLoss.mean": {
            "value": 0.022279693578845277,
            "min": 0.020966119260992854,
            "max": 0.026599229142496673,
            "count": 10
        },
        "MoveToTarget.Losses.PolicyLoss.sum": {
            "value": 0.11139846789422639,
            "min": 0.09280579308591162,
            "max": 0.13299614571248336,
            "count": 10
        },
        "MoveToTarget.Losses.ValueLoss.mean": {
            "value": 0.00032984458307813236,
            "min": 0.00032984458307813236,
            "max": 0.6820954979086915,
            "count": 10
        },
        "MoveToTarget.Losses.ValueLoss.sum": {
            "value": 0.0016492229153906618,
            "min": 0.0016492229153906618,
            "max": 2.728381991634766,
            "count": 10
        },
        "MoveToTarget.Policy.LearningRate.mean": {
            "value": 1.5852094716000004e-05,
            "min": 1.5852094716000004e-05,
            "max": 0.000284613005129,
            "count": 10
        },
        "MoveToTarget.Policy.LearningRate.sum": {
            "value": 7.926047358000001e-05,
            "min": 7.926047358000001e-05,
            "max": 0.0012844260718579998,
            "count": 10
        },
        "MoveToTarget.Policy.Epsilon.mean": {
            "value": 0.10528400000000002,
            "min": 0.10528400000000002,
            "max": 0.19487099999999996,
            "count": 10
        },
        "MoveToTarget.Policy.Epsilon.sum": {
            "value": 0.5264200000000001,
            "min": 0.4994464,
            "max": 0.928142,
            "count": 10
        },
        "MoveToTarget.Policy.Beta.mean": {
            "value": 0.00027367160000000006,
            "min": 0.00027367160000000006,
            "max": 0.0047440629,
            "count": 10
        },
        "MoveToTarget.Policy.Beta.sum": {
            "value": 0.0013683580000000003,
            "min": 0.0013683580000000003,
            "max": 0.0214142858,
            "count": 10
        },
        "MoveToTarget.IsTraining.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 1.0,
            "count": 41
        },
        "MoveToTarget.IsTraining.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 1.0,
            "count": 41
        },
        ".Policy.Entropy.mean": {
            "value": 1.3750606775283813,
            "min": 1.3750606775283813,
            "max": 1.4164819717407227,
            "count": 8
        },
        ".Policy.Entropy.sum": {
            "value": 68731.03125,
            "min": 68731.03125,
            "max": 70872.2578125,
            "count": 8
        },
        ".Environment.EpisodeLength.mean": {
            "value": 203.93333333333334,
            "min": 148.874251497006,
            "max": 216.77489177489178,
            "count": 4
        },
        ".Environment.EpisodeLength.sum": {
            "value": 6118.0,
            "min": 6118.0,
            "max": 50075.0,
            "count": 4
        },
        ".Step.mean": {
            "value": 399938.0,
            "min": 49970.0,
            "max": 399938.0,
            "count": 8
        },
        ".Step.sum": {
            "value": 399938.0,
            "min": 49970.0,
            "max": 399938.0,
            "count": 8
        },
        ".Policy.ExtrinsicValueEstimate.mean": {
            "value": 0.2358882576227188,
            "min": -3.1334359645843506,
            "max": 1.3866379261016846,
            "count": 8
        },
        ".Policy.ExtrinsicValueEstimate.sum": {
            "value": 184.22872924804688,
            "min": -2923.495849609375,
            "max": 1084.350830078125,
            "count": 8
        },
        ".Environment.CumulativeReward.mean": {
            "value": -2.0,
            "min": -2.0,
            "max": -1.896103896103896,
            "count": 4
        },
        ".Environment.CumulativeReward.sum": {
            "value": -62.0,
            "min": -654.0,
            "max": -62.0,
            "count": 4
        },
        ".Policy.ExtrinsicReward.mean": {
            "value": -2.0,
            "min": -2.0,
            "max": -1.896103896103896,
            "count": 4
        },
        ".Policy.ExtrinsicReward.sum": {
            "value": -62.0,
            "min": -654.0,
            "max": -62.0,
            "count": 4
        },
        ".Losses.PolicyLoss.mean": {
            "value": 0.022146538247761784,
            "min": 0.022146538247761784,
            "max": 0.025687014174570023,
            "count": 8
        },
        ".Losses.PolicyLoss.sum": {
            "value": 0.08858615299104713,
            "min": 0.08858615299104713,
            "max": 0.1284350708728501,
            "count": 8
        },
        ".Losses.ValueLoss.mean": {
            "value": 0.0003904953317032778,
            "min": 0.0003904953317032778,
            "max": 1.9725838975359997,
            "count": 8
        },
        ".Losses.ValueLoss.sum": {
            "value": 0.0015619813268131111,
            "min": 0.0015619813268131111,
            "max": 7.890335590143999,
            "count": 8
        },
        ".Policy.LearningRate.mean": {
            "value": 7.461007513e-05,
            "min": 7.461007513e-05,
            "max": 0.0002846116551294501,
            "count": 8
        },
        ".Policy.LearningRate.sum": {
            "value": 0.00029844030052,
            "min": 0.00029844030052,
            "max": 0.0012843144718952,
            "count": 8
        },
        ".Policy.Epsilon.mean": {
            "value": 0.12487000000000001,
            "min": 0.12487000000000001,
            "max": 0.19487054999999998,
            "count": 8
        },
        ".Policy.Epsilon.sum": {
            "value": 0.49948000000000004,
            "min": 0.49948000000000004,
            "max": 0.9281048000000001,
            "count": 8
        },
        ".Policy.Beta.mean": {
            "value": 0.001251013,
            "min": 0.001251013,
            "max": 0.004744040445,
            "count": 8
        },
        ".Policy.Beta.sum": {
            "value": 0.005004052,
            "min": 0.005004052,
            "max": 0.021412429519999995,
            "count": 8
        },
        ".IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 8
        },
        ".IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 8
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1701703238",
        "python_version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\skiry\\UnityRobot\\venv\\Scripts\\mlagents-learn --run-id=Test28",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.1.1+cpu",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1701705674"
    },
    "total": 2436.396852,
    "count": 1,
    "self": 0.007692099999985658,
    "children": {
        "run_training.setup": {
            "total": 0.026907000000000014,
            "count": 1,
            "self": 0.026907000000000014
        },
        "TrainerController.start_learning": {
            "total": 2436.3622529,
            "count": 1,
            "self": 7.083843899950352,
            "children": {
                "TrainerController._reset_env": {
                    "total": 8.3126351,
                    "count": 1,
                    "self": 8.3126351
                },
                "TrainerController.advance": {
                    "total": 2420.8860942000497,
                    "count": 417993,
                    "self": 7.358347100095216,
                    "children": {
                        "env_step": {
                            "total": 2126.312492999987,
                            "count": 417993,
                            "self": 1643.7197552999385,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 478.2316650001022,
                                    "count": 417993,
                                    "self": 35.78441170007716,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 442.44725330002507,
                                            "count": 834288,
                                            "self": 442.44725330002507
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 4.3610726999464955,
                                    "count": 417992,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 2419.245826400084,
                                            "count": 417992,
                                            "is_parallel": true,
                                            "self": 1172.4880272001478,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.00043849999999956424,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.00016580000000043782,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0002726999999991264,
                                                            "count": 4,
                                                            "is_parallel": true,
                                                            "self": 0.0002726999999991264
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 1246.757360699936,
                                                    "count": 417992,
                                                    "is_parallel": true,
                                                    "self": 37.56930049995299,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 50.94184919999951,
                                                            "count": 417992,
                                                            "is_parallel": true,
                                                            "self": 50.94184919999951
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 1051.4474050000035,
                                                            "count": 417992,
                                                            "is_parallel": true,
                                                            "self": 1051.4474050000035
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 106.79880599998009,
                                                            "count": 835984,
                                                            "is_parallel": true,
                                                            "self": 52.207326100079,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 54.59147989990109,
                                                                    "count": 1671968,
                                                                    "is_parallel": true,
                                                                    "self": 54.59147989990109
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 287.21525409996724,
                            "count": 835984,
                            "self": 13.80126009993802,
                            "children": {
                                "process_trajectory": {
                                    "total": 125.180140600029,
                                    "count": 835984,
                                    "self": 125.02350260002908,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 0.1566379999999299,
                                            "count": 4,
                                            "self": 0.1566379999999299
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 148.23385340000024,
                                    "count": 88,
                                    "self": 111.40773650000432,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 36.82611689999592,
                                            "count": 2640,
                                            "self": 36.82611689999592
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 1.500000053056283e-06,
                    "count": 1,
                    "self": 1.500000053056283e-06
                },
                "TrainerController._save_models": {
                    "total": 0.079678199999762,
                    "count": 1,
                    "self": 0.0165634000004502,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.0631147999993118,
                            "count": 2,
                            "self": 0.0631147999993118
                        }
                    }
                }
            }
        }
    }
}